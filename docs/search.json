[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decoding Distinct Mouse Behavior States During Exploration",
    "section": "",
    "text": "1 Introduction\nFor this project, I chose to study mouse behavior using pose-tracking data collected at Columbia University Medical Center’s lab. We recorded ten videos of freely moving mice and tracked several key body parts using DeepLabCut. Two of these mice were part of an experimental (stressed) condition, and the remaining eight were controls. Our main interest is to understand whether early behavioral differences can be detected automatically before more obvious symptoms appear. In other words, can we find subtle posture or movement patterns that distinguish experimental mice from controls? To explore this, I focused on descriptive analysis and visualization. I examined data quality, compared spatial behavior across mice, and used clustering to identify recurring posture states. My goal was not prediction, but discovery: to see whether distinct patterns emerge that could later help identify early signs of brain or stress-related disorders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Technical description\nThe dataset contains tracking outputs from ten videos of mice placed individually in the same circular arena. Each frame includes x/y coordinates and likelihood values for several body parts: nose, ears, head area, tail base, and tail tip. Videos are around 45,000–51,000 frames each (roughly 25–28 minutes at 30 fps).\nAll tracking files were collected directly from our lab’s DeepLabCut pipeline.\nOverall, the data are consistent in format and length across videos, which makes them easy to compare. The only issue was that likelihood for the tail tip was sometimes lower than other body parts, which is common in freely moving animals. Aside from that, the data quality was high.\nCode\nlibrary(tidyverse)\n\nfiles &lt;- list.files(\"data\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\nload_pose &lt;- function(path) {\n  \n  raw &lt;- read_csv(path, col_names = FALSE, show_col_types = FALSE)\n  \n  # DeepLabCut stores headers in rows 1–3:\n  # Row 1 = scorer, Row 2 = bodypart, Row 3 = coordinate\n  header &lt;- raw[1:3, ]\n  data &lt;- raw[-c(1:3), ]\n  \n  bodyparts &lt;- as.character(header[2, ])\n  coords &lt;- as.character(header[3, ])\n  new_names &lt;- paste0(bodyparts, \"_\", coords)\n  \n  colnames(data) &lt;- new_names\n  \n  data &lt;- data |&gt; mutate(across(everything(), as.numeric))\n  \n  data$video_id &lt;- basename(path)\n  data$frame &lt;- seq_len(nrow(data))\n  \n  data\n}\n\npose_all &lt;- map_df(files, load_pose)\n\nglimpse(pose_all)\n\n\nRows: 468,102\nColumns: 24\n$ bodyparts_coords       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ Light_x                &lt;dbl&gt; 838.6320, 839.3212, 839.7064, 842.3026, 845.332…\n$ Light_y                &lt;dbl&gt; 549.5389, 549.8290, 549.8290, 549.5389, 548.463…\n$ Light_likelihood       &lt;dbl&gt; 0.03178143, 0.02934040, 0.03336119, 0.02417329,…\n$ Nose_x                 &lt;dbl&gt; 856.0997, 856.0997, 856.2988, 856.2988, 856.982…\n$ Nose_y                 &lt;dbl&gt; 543.5959, 543.5959, 543.5959, 542.4019, 541.614…\n$ Nose_likelihood        &lt;dbl&gt; 0.9898987, 0.9870993, 0.9825415, 0.9607392, 0.9…\n$ `Tail Base_x`          &lt;dbl&gt; 781.8240, 781.8240, 781.9657, 781.9657, 781.965…\n$ `Tail Base_y`          &lt;dbl&gt; 510.1070, 510.4549, 510.7815, 513.2952, 516.441…\n$ `Tail Base_likelihood` &lt;dbl&gt; 0.9902473, 0.9896467, 0.9900921, 0.9934571, 0.9…\n$ `Tail TIp_x`           &lt;dbl&gt; 738.7238, 738.7238, 738.7238, 738.7238, 738.723…\n$ `Tail TIp_y`           &lt;dbl&gt; 462.0266, 462.1000, 462.2083, 463.1281, 464.436…\n$ `Tail TIp_likelihood`  &lt;dbl&gt; 0.9750594, 0.9725831, 0.9637877, 0.9713903, 0.9…\n$ headarea_x             &lt;dbl&gt; 838.7948, 839.5037, 840.0235, 842.3242, 844.661…\n$ headarea_y             &lt;dbl&gt; 545.5502, 545.5502, 545.5502, 545.2851, 545.285…\n$ headarea_likelihood    &lt;dbl&gt; 0.9974886, 0.9966859, 0.9971533, 0.9958277, 0.9…\n$ leftear_x              &lt;dbl&gt; 837.8909, 838.4118, 838.9759, 841.1564, 843.984…\n$ leftear_y              &lt;dbl&gt; 537.8688, 537.8688, 537.8688, 537.8688, 537.868…\n$ leftear_likelihood     &lt;dbl&gt; 0.9176812, 0.9084605, 0.9133146, 0.8804448, 0.8…\n$ rightear_x             &lt;dbl&gt; 835.9253, 837.0594, 837.4548, 841.0958, 843.624…\n$ rightear_y             &lt;dbl&gt; 554.3290, 554.3290, 554.3290, 553.9014, 553.901…\n$ rightear_likelihood    &lt;dbl&gt; 0.9118637, 0.9150730, 0.9345577, 0.9217122, 0.9…\n$ video_id               &lt;chr&gt; \"Video01_f_c_c6_red.csv\", \"Video01_f_c_c6_red.c…\n$ frame                  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing Value Analysis",
    "text": "2.2 Missing Value Analysis\n\n\nCode\nlibrary(tidyverse)\n\n# Raw DLC variables ending with _x, _y, _likelihood\nraw_vars &lt;- names(pose_all) |&gt;\n  str_subset(\"_(x|y|likelihood)$\")\n\nmissing_by_vid_var &lt;- pose_all |&gt;\n  group_by(video_id) |&gt;\n  summarize(\n    across(all_of(raw_vars), ~ mean(is.na(.))),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = -video_id,\n    names_to = \"variable\",\n    values_to = \"prop_missing\"\n  )\n\nggplot(missing_by_vid_var,\n       aes(x = video_id, y = variable, fill = prop_missing)) +\n  geom_tile() +\n  geom_text(\n    aes(label = ifelse(prop_missing &gt; 0,\n                       scales::percent(prop_missing, accuracy = 0.01),\n                       \"\")),\n    size = 3\n  ) +\n  scale_fill_viridis_c(labels = scales::percent,\n                       name = \"Percent\\nmissing\") +\n  labs(\n    title = \"Missingness by video and variable\",\n    x = \"Video file\",\n    y = \"Variable\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nI checked for missing values across all body parts and videos. According to the heatmap, there were no missing values at all.\n\n\nCode\nlibrary(tidyverse)\n\nraw_vars &lt;- names(pose_all) |&gt;\n  str_subset(\"_(x|y|likelihood)$\")\n\nzero_by_vid_var &lt;- pose_all |&gt;\n  group_by(video_id) |&gt;\n  summarize(\n    across(all_of(raw_vars), ~ mean(. == 0, na.rm = TRUE)),\n    .groups = \"drop\"\n  )|&gt;\n  pivot_longer(\n    cols = -video_id,\n    names_to = \"variable\",\n    values_to = \"prop_zero\"\n  )\n\nggplot(zero_by_vid_var,\n       aes(x = video_id, y = variable, fill = prop_zero)) +\n  geom_tile() +\n  geom_text(\n    aes(label = ifelse(prop_zero &gt; 0,\n                       scales::percent(prop_zero, accuracy = 0.1),\n                       \"\")),\n    size = 3\n  ) +\n  scale_fill_viridis_c(\n    labels = scales::percent,\n    name   = \"Percent\\nzeros\"\n  ) +\n  labs(\n    title = \"Fraction of zero values by video and variable\",\n    x = \"Video file\",\n    y = \"Variable\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nA second plot showed the fraction of zeros, which was extremely small (0.1–0.5%).\n\n\nCode\nlibrary(tidyverse)\n\nlikelihood_vars &lt;- names(pose_all) |&gt;\n  str_subset(\"likelihood$\")\n\n# Compute fraction of frames with likelihood &gt; 0.9\nhigh_conf_by_vid &lt;- pose_all |&gt;\n  group_by(video_id) |&gt;\n  summarize(\n    across(all_of(likelihood_vars), \n           ~ mean(. &gt; 0.9, na.rm = TRUE)),\n    .groups = \"drop\"\n  ) |&gt;\n  pivot_longer(\n    cols = -video_id,\n    names_to = \"variable\",\n    values_to = \"prop_high\"\n  )\n\nggplot(high_conf_by_vid,\n       aes(x = video_id, y = variable, fill = prop_high)) +\n  geom_tile() +\n  geom_text(\n    aes(label = scales::percent(prop_high, accuracy = 0.1)),\n    size = 3\n  ) +\n  scale_fill_viridis_c(\n    labels = scales::percent,\n    name = \"Likelihood &gt; 0.9\"\n  ) +\n  labs(\n    title = \"Fraction of frames with likelihood &gt; 0.9\",\n    x = \"Video file\",\n    y = \"Body part\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nNext, I examined detection confidence. Most likelihood values were above 0.9 for nearly all body parts and videos, except the tail tip. Because the majority of points were reliable, and filtering by likelihood would remove too much data, I kept all frames for analysis.\nTogether, these checks confirmed that the dataset is complete and stable, with no missingness concerns.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nread_pose &lt;- function(path) {\n  readr::read_csv(path, skip = 3, show_col_types = FALSE)\n}\n\ncsv_files &lt;- list.files(\"data\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\nvideo_info &lt;- tibble(\n  file      = csv_files,\n  video_id  = basename(csv_files)\n) |&gt;\n  mutate(\n    data      = map(file, read_pose),\n    n_frames  = map_int(data, nrow),\n    fps       = 30,\n    duration_s = n_frames / fps\n  ) |&gt;\n  arrange(video_id)\n\nvideo_info\n\n\n# A tibble: 10 × 6\n   file                          video_id   data       n_frames   fps duration_s\n   &lt;chr&gt;                         &lt;chr&gt;      &lt;list&gt;        &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 data/Video01_f_c_c6_red.csv   Video01_f… &lt;spc_tbl_&gt;    48638    30      1621.\n 2 data/Video02_m_c_c7_red.csv   Video02_m… &lt;spc_tbl_&gt;    45587    30      1520.\n 3 data/Video03_m_c_c7_black.csv Video03_m… &lt;spc_tbl_&gt;    45963    30      1532.\n 4 data/Video04_f_c_c6_green.csv Video04_f… &lt;spc_tbl_&gt;    50816    30      1694.\n 5 data/Video05_f_c_c6.csv       Video05_f… &lt;spc_tbl_&gt;    46586    30      1553.\n 6 data/Video06_f_c_c5_green.csv Video06_f… &lt;spc_tbl_&gt;    46905    30      1564.\n 7 data/Video07_m_c_c7.csv       Video07_m… &lt;spc_tbl_&gt;    45681    30      1523.\n 8 data/Video08_f_e_c2_red.csv   Video08_f… &lt;spc_tbl_&gt;    46094    30      1536.\n 9 data/Video09_m_e_c3_red.csv   Video09_m… &lt;spc_tbl_&gt;    45940    30      1531.\n10 data/Video10_f_c_c5_red.csv   Video10_f… &lt;spc_tbl_&gt;    45882    30      1529.\n\n\nBefore analyzing posture, I looked at how long each video was. All videos contained a similar number of frames, and their durations only differed by a few minutes. This means any behavioral differences are unlikely to come from recording length.\n\n\nCode\nggplot(video_info, aes(x = video_id, y = n_frames)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Number of frames in each video\",\n    x = \"Video file\",\n    y = \"Number of frames\"\n  ) +\n  theme_minimal(base_size = 16) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(video_info, aes(x = video_id, y = duration_s)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Approximate duration of each video\",\n    x = \"Video file\",\n    y = \"Duration\"\n  ) +\n  theme_minimal(base_size = 16) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\nread_pose_dlc &lt;- function(path) {\n  raw &lt;- read_csv(path, col_names = FALSE, show_col_types = FALSE)\n\n  header  &lt;- raw[1:3, ]\n  data    &lt;- raw[-(1:3), ]\n  bodyparts &lt;- as.character(header[2, ])\n  coords    &lt;- as.character(header[3, ])\n\n  col_names &lt;- paste(bodyparts, coords, sep = \"_\")\n\n  colnames(data) &lt;- col_names\n\n  data &lt;- data |&gt;\n    mutate(across(everything(), as.numeric)) |&gt;\n    mutate(video_id = basename(path), .before = 1)\n\n  data\n}\n\n\n\n\nCode\n# Building video_info with DLC data and group labels\nvideo_info &lt;- tibble(\n  file     = csv_files,\n  video_id = basename(csv_files)\n) |&gt;\n  mutate(\n    data = map(file, read_pose_dlc),\n    n_frames = map_int(data, nrow),\n    fps      = 30,\n    duration_s = n_frames / fps,\n    # 7th and 8th videos are Experimental, others Control\n    group = if_else(row_number() %in% c(7, 8), \"Experimental\", \"Control\")\n  ) |&gt;\n  arrange(video_id)\n\nvideo_info |&gt; select(video_id, group, n_frames, duration_s)\n\n\n# A tibble: 10 × 4\n   video_id                 group        n_frames duration_s\n   &lt;chr&gt;                    &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;\n 1 Video01_f_c_c6_red.csv   Control         48639      1621.\n 2 Video02_m_c_c7_red.csv   Control         45588      1520.\n 3 Video03_m_c_c7_black.csv Control         45964      1532.\n 4 Video04_f_c_c6_green.csv Control         50817      1694.\n 5 Video05_f_c_c6.csv       Control         46587      1553.\n 6 Video06_f_c_c5_green.csv Control         46906      1564.\n 7 Video07_m_c_c7.csv       Experimental    45682      1523.\n 8 Video08_f_e_c2_red.csv   Experimental    46095      1536.\n 9 Video09_m_e_c3_red.csv   Control         45941      1531.\n10 Video10_f_c_c5_red.csv   Control         45883      1529.\n\n\n\n\nCode\n#video_info &lt;- video_info |&gt;\n  #mutate(\n   # data = map(data, ~ .x %&gt;% select(-video_id))\n  #)\n\npose_all &lt;- video_info |&gt;\n  select(group, data) |&gt;\n  unnest(cols = data)\n\nnames(pose_all)\n\n\n [1] \"group\"                \"video_id\"             \"bodyparts_coords\"    \n [4] \"Light_x\"              \"Light_y\"              \"Light_likelihood\"    \n [7] \"Nose_x\"               \"Nose_y\"               \"Nose_likelihood\"     \n[10] \"Tail Base_x\"          \"Tail Base_y\"          \"Tail Base_likelihood\"\n[13] \"Tail TIp_x\"           \"Tail TIp_y\"           \"Tail TIp_likelihood\" \n[16] \"headarea_x\"           \"headarea_y\"           \"headarea_likelihood\" \n[19] \"leftear_x\"            \"leftear_y\"            \"leftear_likelihood\"  \n[22] \"rightear_x\"           \"rightear_y\"           \"rightear_likelihood\" \n\n\n\n\nCode\n# Overall mean\nmean_all &lt;- pose_all |&gt;\n  summarise(\n    group3 = \"All videos\",\n    mean_x = mean(Nose_x, na.rm = TRUE),\n    mean_y = mean(Nose_y, na.rm = TRUE)\n  )\n\n# Control only\nmean_control &lt;- pose_all |&gt;\n  filter(group == \"Control\") |&gt;\n  summarise(\n    group3 = \"Control\",\n    mean_x = mean(Nose_x, na.rm = TRUE),\n    mean_y = mean(Nose_y, na.rm = TRUE)\n  )\n\n# Experimental only\nmean_exp &lt;- pose_all |&gt;\n  filter(group == \"Experimental\") |&gt;\n  summarise(\n    group3 = \"Experimental\",\n    mean_x = mean(Nose_x, na.rm = TRUE),\n    mean_y = mean(Nose_y, na.rm = TRUE)\n  )\n\nmean_positions &lt;- bind_rows(mean_all, mean_control, mean_exp)\nmean_positions\n\n\n# A tibble: 3 × 3\n  group3       mean_x mean_y\n  &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1 All videos     614.   340.\n2 Control        613.   342.\n3 Experimental   620.   335.\n\n\n\n\nCode\nggplot() +\n  geom_point(\n    data = pose_all,\n    aes(x = Nose_x, y = Nose_y),\n    color = \"grey80\",\n    alpha = 0.05,\n    size = 0.3\n  ) +\n  geom_point(\n    data = mean_positions,\n    aes(x = mean_x, y = mean_y, color = group3),\n    size = 4\n  ) +\n  geom_text(\n    data = mean_positions,\n    aes(x = mean_x, y = mean_y, label = group3, color = group3),\n    nudge_y = 20,\n    show.legend = FALSE\n  ) +\n  labs(\n    title = \"Mean nose position\",\n    x = \"Nose_x\",\n    y = \"Nose_y\",\n    color = \"Category\"\n  ) +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nI began by comparing raw nose positions across all videos. When I plotted average nose location for: all mice combined, control mice only, and experimental mice only, all three points were almost on top of each other, near the center of the arena.\n\n\nCode\nggplot(pose_all, aes(x = Nose_x, y = Nose_y)) +\n  geom_point(aes(color = group),\n             alpha = 0.05, size = 0.3) +\n  coord_equal() +\n  facet_wrap(~ group) +\n  labs(\n    title = \"Posture distribution in camera coordinates\",\n    x = \"Nose_x\",\n    y = \"Nose_y\",\n    color = \"Group\"\n  ) +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nI then compared full posture clouds (all frames), showing where each mouse’s nose appeared in camera coordinates. Both control and experimental mice formed nearly identical “X-shaped” patterns. This shape comes from the geometry of the arena and the way mice move across it. In raw camera coordinates, experimental mice do not appear to explore the arena differently than controls. This suggests we should look deeper, at body-relative posture rather than position in space.\n\n4 Relative posture and clustering\nTo study posture itself, I converted body-part locations into a coordinate system relative to the mouse’s own head–body orientation. This removes the influence of where the mouse is in the arena and focuses on the shape and angles of the body. I then ran PCA for visualization and applied k-means clustering to group similar postures.\n\n\nCode\n#install.packages(\"janitor\")\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(janitor)\n\npose_all &lt;- pose_all |&gt; clean_names()\n\nparts &lt;- c(\"nose\", \"tail_base\", \"headarea\", \"leftear\", \"rightear\", \"light\")\navailable_parts &lt;- parts[paste0(parts, \"_x\") %in% names(pose_all)]\n\n# Body midpoint and length\npose_rel &lt;- pose_all |&gt;\n  mutate(\n    origin_x = (nose_x + tail_base_x) / 2,\n    origin_y = (nose_y + tail_base_y) / 2,\n    body_len = sqrt((nose_x - tail_base_x)^2 + (nose_y - tail_base_y)^2),\n    body_len_safe = if_else(body_len &gt; 0, body_len, NA_real_)\n  )\n\n# Creating relative coordinates\nfor (p in available_parts) {\n  pose_rel[[paste0(p, \"_rel_x\")]] &lt;- (pose_rel[[paste0(p, \"_x\")]] - pose_rel$origin_x) / pose_rel$body_len_safe\n  pose_rel[[paste0(p, \"_rel_y\")]] &lt;- (pose_rel[[paste0(p, \"_y\")]] - pose_rel$origin_y) / pose_rel$body_len_safe\n}\n\npose_rel &lt;- pose_rel |&gt; filter(!is.na(nose_rel_x))\n\n\n\n\nCode\nrel_cols &lt;- grep(\"_rel_\", names(pose_rel), value = TRUE)\n\nX &lt;- pose_rel |&gt; select(all_of(rel_cols)) |&gt; as.matrix()\n\n\n\n\nCode\nset.seed(42)\n\nkm &lt;- kmeans(X, centers = 10, nstart = 20)\n\npose_rel$cluster &lt;- factor(km$cluster)\n\n\n\n\nCode\npca &lt;- prcomp(X, scale. = TRUE)\npca_df &lt;- data.frame(\n  PC1 = pca$x[,1],\n  PC2 = pca$x[,2],\n  cluster = pose_rel$cluster\n)\n\nggplot(pca_df, aes(PC1, PC2, color = cluster)) +\n  geom_point(alpha = 0.3, size = 0.7) +\n  labs(title = \"K-means clusters on relative posture (PCA view)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nThe PCA scatterplot shows that some clusters are tight and well-separated, while others are more spread out. This means: certain postures are very consistent, other postures act more like transitions between main states. The PCA plot also confirms that relative posture reveals structure that is completely invisible in raw camera coordinates.\n\n\nCode\ncluster_fraction &lt;- pose_rel |&gt;\n  group_by(video_id, cluster) |&gt;\n  summarise(n = n()) |&gt;\n  group_by(video_id) |&gt;\n  mutate(p = n / sum(n))\n\nggplot(cluster_fraction,\n       aes(x = video_id, y = p, fill = cluster)) +\n  geom_col() +\n  geom_text(aes(label = scales::percent(p, accuracy = 1)),\n            position = position_stack(vjust = 0.5), size = 3) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Fraction of frames in each posture cluster by video\",\n       x = \"Video\", y = \"Percent of frames\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe stacked bar charts show that:all videos contain all posture clusters, but the proportion of each cluster varies somewhat from mouse to mouse. Importantly, the experimental mice do not show any cluster that is completely absent in controls. Instead, they may differ in how much time they spend in certain states.\n\n\nCode\nggplot(pose_rel, aes(x = cluster, y = nose_rel_y, fill = cluster)) +\n  geom_boxplot(outlier.size = 0.5) +\n  labs(title = \"Nose vertical position by cluster\",\n       y = \"Nose_rel_y (body lengths)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nTo better understand the meaning of each cluster, I plotted the relative vertical nose position for every cluster. Clusters clearly separate into:head-up postures (higher nose values), head-down postures (lower nose values), and mid-level postures. This interpretation is helpful because it translates the clusters into something biologically meaningful.\n\n\nCode\npose_rel &lt;- pose_rel |&gt;\n  group_by(video_id) |&gt;\n  arrange(video_id) |&gt;\n  mutate(frame = row_number()) |&gt;\n  ungroup()\n\npose_rel$cluster &lt;- as.factor(pose_rel$cluster)\n\nvideo_to_plot &lt;- unique(pose_rel$video_id)[1]\n\nv1 &lt;- pose_rel |&gt;\n  filter(video_id == video_to_plot)\n\nggplot(v1, aes(x = frame, y = 1, fill = cluster)) +\n  geom_tile() +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = paste(\"Sequence of posture clusters over time\", video_to_plot),\n    x = \"Frame\",\n    y = \"\",\n    fill = \"Cluster\"\n  ) +\n  theme_minimal(base_size = 18) +\n  theme(\n    axis.text.y  = element_blank(),\n    axis.ticks.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nVisualizing the cluster sequence over time shows that mice switch posture very frequently. This is expected for freely moving animals exploring a new environment.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Fraction of frames in each cluster, by group\ncluster_fraction_group &lt;- pose_rel |&gt;\n  group_by(group, cluster) |&gt;\n  summarise(n = n(), .groups = \"drop_last\") |&gt;\n  mutate(p = n / sum(n)) |&gt;\n  ungroup()\n\ncluster_fraction_group\n\n\n# A tibble: 19 × 4\n   group        cluster     n          p\n   &lt;chr&gt;        &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Control      1           2 0.00000532\n 2 Control      2         326 0.000868  \n 3 Control      3       57458 0.153     \n 4 Control      4       33078 0.0881    \n 5 Control      5       30408 0.0809    \n 6 Control      6       53960 0.144     \n 7 Control      7       58271 0.155     \n 8 Control      8       44066 0.117     \n 9 Control      9       98078 0.261     \n10 Control      10         15 0.0000399 \n11 Experimental 2           7 0.0000763 \n12 Experimental 3       17600 0.192     \n13 Experimental 4        5992 0.0653    \n14 Experimental 5        7717 0.0841    \n15 Experimental 6       13655 0.149     \n16 Experimental 7       15505 0.169     \n17 Experimental 8        9594 0.105     \n18 Experimental 9       21636 0.236     \n19 Experimental 10          1 0.0000109 \n\n\nCode\nggplot(cluster_fraction_group,\n       aes(x = group, y = p, fill = cluster)) +\n  geom_col() +\n  geom_text(aes(label = percent(p, accuracy = 1)),\n            position = position_stack(vjust = 0.5),\n            size = 4) +\n  scale_y_continuous(labels = percent_format()) +\n  labs(\n    title = \"Fraction of frames in each posture cluster\",\n    x = \"Group\",\n    y = \"Percent of frames\",\n    fill = \"Cluster\"\n  ) +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nThis plot summarizes how often each posture cluster appears in the two groups. Noticeable differences appear in several clusters: Cluster 3 and 7 occur slightly more often in the Experimental group.Cluster 4 and 9 occur more often in Control mice. These small but consistent shifts suggest that stressed mice may bias toward certain posture states.\n\n\nCode\ncluster_diff &lt;- cluster_fraction_group |&gt;\n  select(group, cluster, p) |&gt;\n  tidyr::pivot_wider(names_from = group, values_from = p) |&gt;\n  mutate(diff_exp_minus_ctrl = Experimental - Control)\n\nggplot(cluster_diff,\n       aes(x = cluster, y = diff_exp_minus_ctrl, fill = diff_exp_minus_ctrl &gt; 0)) +\n  geom_col() +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"TRUE\" = \"tomato\", \"FALSE\" = \"steelblue\"),\n                    labels = c(\"FALSE\" = \"More in Control\",\n                               \"TRUE\"  = \"More in Experimental\"),\n                    name = \"\") +\n  labs(\n    title = \"Difference in cluster usage\",\n    x = \"Cluster\",\n    y = \"Difference in percent of frames\"\n  ) +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\n\n\n\nThis graph makes the group differences easier to interpret by showing the percent-difference directly.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\n\n# All successive transitions within each video\ntransitions &lt;- pose_rel |&gt;\n  arrange(video_id, frame) |&gt;\n  group_by(video_id) |&gt;\n  mutate(cluster_next = dplyr::lead(cluster)) |&gt;\n  ungroup() |&gt;\n  filter(!is.na(cluster_next))\n\n# Count transitions by group\ntrans_mat_group &lt;- transitions |&gt;\n  group_by(group, cluster, cluster_next) |&gt;\n  summarise(n = n(), .groups = \"drop_last\") |&gt;\n  mutate(p = n / sum(n)) |&gt; \n  ungroup()\n\n# Heatmap for each group\nggplot(trans_mat_group,\n       aes(x = cluster, y = cluster_next, fill = p)) +\n  geom_tile() +\n  facet_wrap(~ group) +\n  scale_fill_gradient(low = \"white\", high = \"darkred\",\n                      labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Transition probabilities between posture clusters\",\n    x = \"From cluster\",\n    y = \"To cluster\",\n    fill = \"Probability\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nWhen I looked at how mice move from one posture cluster to another, both groups mostly stayed in the same posture from frame to frame, which is expected. But I did notice one small difference: the Experimental mice shifted into cluster 3 a bit more often, while the Control mice tended to move into clusters 4 and 9 more frequently. So even though the overall transition patterns look similar, the stressed mice seem to favor slightly different movement patterns.\n\n\nCode\nrle_dwell &lt;- pose_rel |&gt;\n  arrange(video_id, frame) |&gt;\n  group_by(video_id) |&gt;\n  do({\n    cl  &lt;- .$cluster\n    gr  &lt;- .$group[1]\n    r   &lt;- rle(as.character(cl))\n    tibble(\n      group     = gr,\n      cluster   = factor(r$values, levels = levels(.$cluster)),\n      dwell_len = r$lengths / 30\n    )\n  }) |&gt;\n  ungroup()\n\nggplot(rle_dwell,\n       aes(x = cluster, y = dwell_len, fill = group)) +\n  geom_boxplot(outlier.size = 0.4, position = position_dodge(width = 0.7)) +\n  labs(\n    title = \"Dwell time in each posture cluster\",\n    x = \"Cluster\",\n    y = \"Dwell time (seconds)\",\n    fill = \"Group\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nI also checked how long mice stay in each posture before switching. Most postures were held only briefly, but the Experimental mice tended to remain in clusters 3 and 7 a bit longer, and spent less time in clusters that look more like typical locomotion. This again hints that stress may change how mice behave moment-to-moment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#conclusion",
    "href": "results.html#conclusion",
    "title": "3  Results",
    "section": "4.1 Conclusion",
    "text": "4.1 Conclusion\nOverall, this exploration showed that although Control and Experimental mice share the same general set of posture clusters, they use them in slightly different ways - Experimental mice spend more time in certain clusters and transition into them more often, which may reflect early behavioral changes related to stress. At the same time, our analysis has some limitations: we worked with a relatively small number of videos, relied on unsupervised clustering, and did not yet validate these posture groups with expert behavioral labels. In the future, I would like to incorporate more videos, try alternative embedding methods like UMAP or CEBRA, and compare our clusters to real behavioral categories such as grooming or rearing. Through this project, I learned how powerful pose-tracking data can be, but also how careful we need to be when interpreting subtle differences, especially when working with small datasets and unsupervised methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  }
]